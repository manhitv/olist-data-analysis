{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152fd1ab-e8e8-4421-be2f-48753e6afd80",
   "metadata": {},
   "source": [
    "# Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e64f40d-e8d0-45e0-9fc0-f17fc71a3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a map for location of customers (optional: with sellers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3dfeb-e74e-41b4-83f8-8cfde465158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer throgh time \n",
    "# The graph shows, in 2017, there has been a positive trend line in number of New Customers [Customer Unique Identity] getting registered with Olist. In 2018, more than 6000 were getting registered every month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563ec35-0592-4b16-ac90-eb68b3291cc7",
   "metadata": {},
   "source": [
    "## 1. Cohort Analysis\n",
    "A cohort is a group of users who share something in common, be it their sign-up date, first purchase, Interests, acquisition channel, etc. Cohort analysis is the method by which these groups are tracked over time, helping you spot trends, understand repeat behaviors (purchases, engagement, amount spent, etc.), and monitor your customer and revenue retention.\n",
    "\n",
    "When building a cohort analysis, it’s important to consider the relationship between the event or interaction you’re tracking and its relationship to your business model.\n",
    "\n",
    "TODO: Learn more about this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7418f7-06ed-4cc5-9e7f-54e5daffa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the user's cohort group (based on their first order)\n",
    "# Create a new column called CohortGroup, which is the year and month in which the user's first purchase occurred.\n",
    "df = Olist_db[['customer_unique_id', 'order_id', 'order_purchase_timestamp', 'product_category_name', 'customer_state']].drop_duplicates()\n",
    "df['OrderMonth'] = df['order_purchase_timestamp'].dt.to_period('M')\n",
    "df['CohortGroup'] = df.groupby('customer_unique_id')['order_purchase_timestamp'] \\\n",
    "                 .transform('min') \\\n",
    "                 .dt.to_period('M') \n",
    "from operator import attrgetter\n",
    "import matplotlib.colors as mcolors\n",
    "df_cohort = df.groupby(['CohortGroup', 'OrderMonth']) \\\n",
    "              .agg(n_customers=('customer_unique_id', 'count')) \\\n",
    "              .reset_index(drop=False)\n",
    "df_cohort['PeriodIndex'] = (df_cohort.OrderMonth - df_cohort.CohortGroup).apply(attrgetter('n'))\n",
    "\n",
    "def get_data(df_cohort):\n",
    "    cohort_pivot = df_cohort.pivot_table(index = 'CohortGroup',\n",
    "                                     columns = 'PeriodIndex',\n",
    "                                     values = 'n_customers')\n",
    "    #cohort_pivot.iloc[:,0]= (cohort_pivot.iloc[:,0].divide(25, axis = 0)).astype(int)\n",
    "    #cohort_pivot.iloc[:,1:] = cohort_pivot.iloc[:,0:].divide(2, axis = 0)\n",
    "    cohort_pivot = cohort_pivot.iloc[4:-1,:-5]\n",
    "    return cohort_pivot\n",
    "\n",
    "cohort_pivot_ = get_data(df_cohort)\n",
    "cohort_pivot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913be3c-6e7b-4e83-82bc-853e5f631e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_size = cohort_pivot_.iloc[:,0]\n",
    "retention_matrix = cohort_pivot_.divide(cohort_size, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d7308-f816-4190-8c3c-7afd5f6ba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 8), sharey=True, gridspec_kw={'width_ratios': [1, 20]})\n",
    "    \n",
    "    # retention matrix\n",
    "    sns.heatmap(retention_matrix, \n",
    "                mask=retention_matrix.isnull(), \n",
    "                annot=True, \n",
    "                fmt='.0%', \n",
    "                cmap='RdYlGn', \n",
    "                ax=ax[1])\n",
    "    ax[1].set_title('Monthly Cohorts: User Retention', fontsize=16)\n",
    "    ax[1].set(xlabel='# of periods',\n",
    "              ylabel='')\n",
    "\n",
    "    # cohort size\n",
    "    cohort_size_df = pd.DataFrame(cohort_size).rename(columns={0: 'cohort_size'})\n",
    "    white_cmap = mcolors.ListedColormap(['white'])\n",
    "    sns.heatmap(cohort_size_df, \n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                fmt='g', \n",
    "                #cmap=white_cmap, \n",
    "                ax=ax[0])\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fefb6-7f49-4a67-97aa-c10dab610c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Retention by state\n",
    "def retention_by_state(state, df):\n",
    "    \n",
    "    df = df[df.customer_state == state]\n",
    "\n",
    "    df_cohort = df.groupby(['CohortGroup', 'OrderMonth']) \\\n",
    "                  .agg(n_customers=('customer_unique_id', 'count')) \\\n",
    "                  .reset_index(drop=False)\n",
    "    df_cohort['PeriodIndex'] = (df_cohort.OrderMonth - df_cohort.CohortGroup).apply(attrgetter('n'))\n",
    "\n",
    "    def get_data(df_cohort):\n",
    "        cohort_pivot = df_cohort.pivot_table(index = 'CohortGroup',\n",
    "                                         columns = 'PeriodIndex',\n",
    "                                         values = 'n_customers')\n",
    "        #cohort_pivot.iloc[:,0]= (cohort_pivot.iloc[:,0].divide(25, axis = 0)).astype(int)\n",
    "        #cohort_pivot.iloc[:,1:] = cohort_pivot.iloc[:,0:].divide(2, axis = 0)\n",
    "        cohort_pivot = cohort_pivot.iloc[4:-1,:-5]\n",
    "        return cohort_pivot\n",
    "\n",
    "    cohort_pivot_ = get_data(df_cohort)\n",
    "    cohort_pivot_\n",
    "\n",
    "    cohort_size = cohort_pivot_.iloc[:,0]\n",
    "    retention_matrix = cohort_pivot_.divide(cohort_size, axis = 0)\n",
    "\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 8), sharey=True, gridspec_kw={'width_ratios': [1, 20]})\n",
    "\n",
    "        # retention matrix\n",
    "        sns.heatmap(retention_matrix, \n",
    "                    mask=retention_matrix.isnull(), \n",
    "                    annot=True, \n",
    "                    fmt='.0%', \n",
    "                    cmap='RdYlGn', \n",
    "                    ax=ax[1])\n",
    "        ax[1].set_title('Monthly Cohorts: User Retention', fontsize=16)\n",
    "        ax[1].set(xlabel='# of periods',\n",
    "                  ylabel='')\n",
    "\n",
    "        \n",
    "        # cohort size\n",
    "        cohort_size_df = pd.DataFrame(cohort_size).rename(columns={0: 'cohort_size'})\n",
    "        white_cmap = mcolors.ListedColormap(['white'])\n",
    "        sns.heatmap(cohort_size_df, \n",
    "                    annot=True, \n",
    "                    cbar=False, \n",
    "                    fmt='g', \n",
    "                    #cmap=white_cmap, \n",
    "                    ax=ax[0])\n",
    "\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad98d05-b0fa-488f-b694-261fd3a3ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customer_state.unique()\n",
    "retention_by_state('RJ', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d99f7-124c-4f1a-9712-d91a10024a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will see this chart again in the dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efda6b-6f7a-44d1-b027-08dc6459a318",
   "metadata": {},
   "source": [
    "## 2. Customer Segmentation\n",
    "We discovered some interesting observations around our cohort data set. While cohort analysis provides us with customer behavior overtime and understand retention rates, we also want to be able to segment our data by their behavior as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa160de-a79d-4605-86ff-54678dc44fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate some overlapped content with EDA\n",
    "# 2. Customer Type through time (new, existing, total) \n",
    "# 3. Customers vs. locations: city / state vs. sellers' location (tend to same state, same city)\n",
    "# 3. Customer value: price / freight value / payment type\n",
    "# 4. Calculate customer retention rate & visualize distribution -- google or refer https://github.com/Nikhilkohli1/Olist-Marketing-Analytics/blob/master/4.%20Uplift%20Modeling/Olist_Customer_Lifetime_Value_.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4c2a7-3fcc-45e7-bead-071275322630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for dataframes\n",
    "import matplotlib.pyplot as plt # for plotting graphs\n",
    "import seaborn as sns # for plotting graphs\n",
    "import datetime as dt\n",
    "import re\n",
    "from pandas.plotting import scatter_matrix\n",
    "import time, warnings\n",
    "import datetime as dt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055d261-d383-4f1f-a301-5ad6db06775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_plotly_browser_state():\n",
    "    import IPython\n",
    "    display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094f71b-0aac-4439-b95d-5af781a0a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "Olist_db = Olist_db.drop_duplicates()\n",
    "cond = Olist_db[\"order_status\"] == \"delivered\"\n",
    "Olist_db = Olist_db.loc[cond]\n",
    "def clustering_customers(df, date_max, date_min=False, group_range_days=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    if(date_min == False):\n",
    "        cond_f = df[\"order_purchase_date\"] <= pd.to_datetime(date_max)\n",
    "    else:\n",
    "        cond_1 = df[\"order_purchase_date\"] <= pd.to_datetime(date_max)\n",
    "        cond_2 = df[\"order_purchase_date\"] >= pd.to_datetime(date_min)\n",
    "        cond_f = cond_1 & cond_2\n",
    "\n",
    "    df = df.loc[cond_f]\n",
    "\n",
    "    df[\"today\"] = df[\"order_purchase_date\"].max()\n",
    "\n",
    "    df[\"today\"] = df[\"today\"].dt.date\n",
    "    df[\"today\"] = pd.to_datetime(df[\"today\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    df[\"order_purchase_days_since\"] = df[\"today\"]  - df[\"order_purchase_date\"]\n",
    "    df[\"order_purchase_days_since\"] = df[\"order_purchase_days_since\"].astype(str)\n",
    "    df[\"order_purchase_days_since\"] = df[\"order_purchase_days_since\"].str.replace(r'\\s+days.*', '', regex=True)\n",
    "    df[\"order_purchase_days_since\"] = df[\"order_purchase_days_since\"].astype(int)\n",
    "    df[\"order_purchase_year\"] = df[\"order_purchase_date\"].dt.year\n",
    "\n",
    "    agg_group = {\n",
    "        \"order_purchase_days_since\": [\"min\", \"max\", \"count\"],\n",
    "        \"payment_value\": [\"sum\",\"mean\"]\n",
    "    }\n",
    "\n",
    "    df_group = df.groupby([\"customer_unique_id\"]).agg(agg_group).reset_index()\n",
    "\n",
    "    df_group.columns = [' '.join(col).strip() for col in df_group.columns.values]\n",
    "\n",
    "    columns_rename = {\n",
    "        \"order_purchase_days_since min\": \"first_order_purchase\",\n",
    "        \"order_purchase_days_since max\": \"last_order_purchase\",\n",
    "        \"order_purchase_days_since count\": \"order_purchase_qty\",\n",
    "        \"payment_value mean\": \"payment_value_mean\",\n",
    "        \"payment_value sum\": \"payment_value_sum\"\n",
    "    }\n",
    "\n",
    "    df_group.rename(columns_rename, axis=1, inplace=True)\n",
    "\n",
    "    median_payment = df_group[\"payment_value_mean\"].median()\n",
    "\n",
    "    if(group_range_days == False):\n",
    "        major_group = 4\n",
    "        range_days = str(df[\"order_purchase_date\"].max() - df[\"order_purchase_date\"].min()) \n",
    "        group_range_days = int(re.sub(r'\\s+days.*', '', range_days))/major_group\n",
    "\n",
    "    cond_payment_zero = df_group['payment_value_mean'] == 0.0\n",
    "    \n",
    "    # Explain here\n",
    "    cond_inactive_1 = df_group['last_order_purchase'] > group_range_days*3\n",
    "    cond_inactive = cond_inactive_1 | cond_payment_zero\n",
    "    \n",
    "    # Explain here\n",
    "    cond_cold_1 = df_group['last_order_purchase'] > group_range_days*2\n",
    "    cond_cold_2 = df_group['last_order_purchase'] <= group_range_days*3\n",
    "    cond_cold = cond_cold_1 & cond_cold_2 & ~(cond_payment_zero)\n",
    "    \n",
    "    # Explain here\n",
    "    cond_hot_1 = df_group['last_order_purchase'] > group_range_days\n",
    "    cond_hot_2 = df_group['last_order_purchase'] <= group_range_days*2\n",
    "    cond_hot = cond_hot_1 & cond_hot_2 & ~(cond_payment_zero)\n",
    "    \n",
    "    # Explain here\n",
    "    cond_active_1 = df_group['last_order_purchase'] <= group_range_days\n",
    "    cond_active = cond_active_1 & ~(cond_payment_zero)\n",
    "\n",
    "    df_group.loc[cond_inactive, \"segment\"] = \"inactive\"\n",
    "    df_group.loc[cond_cold, \"segment\"] = \"cold\"\n",
    "    df_group.loc[cond_hot, \"segment\"] = \"hot\"\n",
    "    df_group.loc[cond_active, \"segment\"] = \"active\"\n",
    "    \n",
    "    cond_hot_high_payment_1 = df_group[\"segment\"] == \"hot\"\n",
    "    cond_hot_high_payment_2 = df_group[\"payment_value_mean\"] >= median_payment\n",
    "    cond_hot_high_payment = cond_hot_high_payment_1 & cond_hot_high_payment_2\n",
    "    \n",
    "    cond_hot_low_payment_1 = df_group[\"segment\"] == \"hot\"\n",
    "    cond_hot_low_payment_2 = df_group[\"payment_value_mean\"] < median_payment\n",
    "    cond_hot_low_payment = cond_hot_low_payment_1 & cond_hot_low_payment_2\n",
    "    \n",
    "    cond_active_high_payment_1 = df_group[\"segment\"] == \"active\"\n",
    "    cond_active_high_payment_2 = df_group[\"payment_value_mean\"] >= median_payment\n",
    "    cond_active_high_payment = cond_active_high_payment_1 & cond_active_high_payment_2\n",
    "\n",
    "    cond_active_low_payment_1 = df_group[\"segment\"] == \"active\"\n",
    "    cond_active_low_payment_2 = df_group[\"payment_value_mean\"] < median_payment\n",
    "    cond_active_low_payment = cond_active_low_payment_1 & cond_active_low_payment_2\n",
    "\n",
    "    cond_cold_high_payment_1 = df_group[\"segment\"] == \"cold\"\n",
    "    cond_cold_high_payment_2 = df_group[\"payment_value_mean\"] >= median_payment\n",
    "    cond_cold_high_payment = cond_cold_high_payment_1 & cond_cold_high_payment_2\n",
    "\n",
    "    cond_cold_low_payment_1 = df_group[\"segment\"] == \"cold\"\n",
    "    cond_cold_low_payment_2 = df_group[\"payment_value_mean\"] < median_payment\n",
    "    cond_cold_low_payment = cond_cold_low_payment_1 & cond_cold_low_payment_2\n",
    "\n",
    "    df_group[\"sub_segment\"] = \"inactive\"\n",
    "    df_group.loc[cond_hot_high_payment, \"sub_segment\"] = \"hot_high_payment_value\"\n",
    "    df_group.loc[cond_hot_low_payment, \"sub_segment\"] = \"hot_low_payment_value\"\n",
    "    df_group.loc[cond_active_high_payment, \"sub_segment\"] = \"active_high_payment_value\"\n",
    "    df_group.loc[cond_active_low_payment, \"sub_segment\"] = \"active_low_payment_value\"\n",
    "    df_group.loc[cond_cold_high_payment, \"sub_segment\"] = \"cold_high_payment_value\"\n",
    "    df_group.loc[cond_cold_low_payment, \"sub_segment\"] = \"cold_low_payment_value\"\n",
    "\n",
    "    cond_new_customer = df_group[\"first_order_purchase\"] <= group_range_days*2\n",
    "    df_group[\"new_customer\"] = 0\n",
    "    df_group.loc[cond_new_customer, \"new_customer\"] = 1\n",
    "    \n",
    "    return group_range_days, df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eaf1a2-a466-4c74-95b2-51e6e111d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_2018, df_clustering_2018 = clustering_customers(Olist_db, \"2018-12-31\")\n",
    "df_clustering_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ab340-f331-4078-9ac8-e72a21a3a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revenue_subsegment_2018 = df_clustering_2018.groupby([\"sub_segment\"]).agg({\"payment_value_sum\": \"sum\"}).reset_index()\n",
    "\n",
    "configure_plotly_browser_state()\n",
    "trace0 = go.Bar(\n",
    "    x=df_revenue_subsegment_2018[\"sub_segment\"].values,\n",
    "    y=df_revenue_subsegment_2018[\"payment_value_sum\"].values,\n",
    "    marker=dict(\n",
    "        color=['rgba(36,123,160,1)', \n",
    "               'rgba(75,147,177,1)',\n",
    "               'rgba(112,193,179,1)', \n",
    "               'rgba(138,204,192,1)',\n",
    "               'rgba(243,255,189,1)',\n",
    "               'rgba(247,255,213,1)',\n",
    "               'rgba(255,22,84,1)']),\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Revenue 2018',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)\n",
    "df_clustering_2018_qty = df_clustering_2018.loc[df_clustering_2018[\"order_purchase_qty\"] > 1]\n",
    "\n",
    "df_qty_subsegment_2018 = df_clustering_2018_qty.groupby([\"sub_segment\"]).agg({\"order_purchase_qty\": \"count\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687a494-6c4e-4611-93f7-ac162e5f25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()\n",
    "trace0 = go.Bar(\n",
    "    x=df_qty_subsegment_2018[\"sub_segment\"].values,\n",
    "    y=df_qty_subsegment_2018[\"order_purchase_qty\"].values,\n",
    "    marker=dict(\n",
    "        color=['rgba(36,123,160,1)', \n",
    "               'rgba(75,147,177,1)',\n",
    "               'rgba(112,193,179,1)', \n",
    "               'rgba(138,204,192,1)',\n",
    "               'rgba(243,255,189,1)',\n",
    "               'rgba(247,255,213,1)',\n",
    "               'rgba(255,22,84,1)']),\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Repurchase Amount 2018',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e10858-3047-496e-92d6-a3173cb0887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days_repurchase_subsegment_2018 = df_clustering_2018_qty.groupby([\"sub_segment\"]).agg({\"first_order_purchase\": \"mean\", \"last_order_purchase\": \"mean\"}).reset_index()\n",
    "\n",
    "df_days_repurchase_subsegment_2018[\"diff_order_purchase\"] = df_days_repurchase_subsegment_2018[\"last_order_purchase\"].values - df_days_repurchase_subsegment_2018[\"first_order_purchase\"].values\n",
    "\n",
    "df_days_repurchase_subsegment_2018[\"diff_order_purchase\"] = df_days_repurchase_subsegment_2018[\"diff_order_purchase\"].round(0)\n",
    "configure_plotly_browser_state()\n",
    "trace0 = go.Bar(\n",
    "    x=df_days_repurchase_subsegment_2018[\"sub_segment\"].values,\n",
    "    y=df_days_repurchase_subsegment_2018[\"diff_order_purchase\"].values,\n",
    "    marker=dict(\n",
    "        color=['rgba(36,123,160,1)', \n",
    "               'rgba(75,147,177,1)',\n",
    "               'rgba(112,193,179,1)', \n",
    "               'rgba(138,204,192,1)',\n",
    "               'rgba(243,255,189,1)',\n",
    "               'rgba(247,255,213,1)',\n",
    "               'rgba(255,22,84,1)']),\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Avg days between first and last purchase',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24683c-ca8b-4043-b8c8-4178186cd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_2017, df_clustering_2017 = clustering_customers(Olist_db, \"2017-12-31\")\n",
    "\n",
    "df_clustering_2017.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea065ae4-3ce4-45a2-8bee-aa43bdc3588b",
   "metadata": {},
   "source": [
    "## 3. RFM\n",
    "Behavioral segmentation by 3 important features:\n",
    "- Recency — number of days since the last purchase\n",
    "- Frequency — number of transactions made over a given period\n",
    "- Monetary — amount spent over a given period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ec5c2-fdf7-4ded-8bdb-8765d29ddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import squarify\n",
    "Olist_db.columns\n",
    "Index(['order_id', 'order_status', 'order_purchase_timestamp', 'payment_value',\n",
    "       'customer_unique_id', 'customer_zip_code_prefix', 'customer_city',\n",
    "       'customer_state', 'order_purchase_date'],\n",
    "      dtype='object')\n",
    "Olist_db['InvoiceDate'] = Olist_db.order_purchase_date.apply(lambda x : datetime.strftime(x, '%Y-%m-%d'))\n",
    "Olist_db['InvoiceDate'] = pd.to_datetime(Olist_db['InvoiceDate'])\n",
    "print('{:,} rows; {:,} columns'\n",
    "      .format(Olist_db.shape[0], Olist_db.shape[1]))\n",
    "print('{:,} transactions don\\'t have a customer id'\n",
    "      .format(Olist_db[Olist_db.customer_unique_id.isnull()].shape[0]))\n",
    "print('Transactions timeframe from {} to {}'.format(Olist_db['InvoiceDate'].min(),\n",
    "                                    Olist_db['InvoiceDate'].max()))\n",
    "98,770 rows; 10 columns\n",
    "0 transactions don't have a customer id\n",
    "Transactions timeframe from 2016-10-03 00:00:00 to 2018-08-29 00:00:00\n",
    "# Create snapshot date\n",
    "snapshot_date = Olist_db['InvoiceDate'].max() + timedelta(days=1)\n",
    "snapshot_date\n",
    "Timestamp('2018-08-30 00:00:00')\n",
    "# Grouping by CustomerID\n",
    "data_process = Olist_db.groupby(['customer_unique_id']).agg({\n",
    "        'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "        'order_id': 'count',\n",
    "        'payment_value': 'sum'})\n",
    "data_process.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'order_id': 'Frequency',\n",
    "                         'payment_value': 'Monetary'}, inplace=True)\n",
    "data_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5725bd0-229f-4eea-b619-965a3c60d71f",
   "metadata": {},
   "source": [
    "Great, we have 41,431 customer records grouped by recency of their purchase, the frequency by their quantity, and the monetary value of the purchases. Now we can get into the meat of things and use the .qcut() method to assign the relative percentile to their RFM features. But before that, let’s examine the distribution of our Recency, Frequency, and Monetary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a3834-a561-4335-8acb-7bcf99cdc751",
   "metadata": {},
   "source": [
    "#### 3.1. Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bacadc-2c2d-4b3d-900e-caa4c5514814",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "# Plot distribution of Recency\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.distplot(data_process['Recency'])\n",
    "# Plot distribution of Frequency\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.distplot(data_process['Frequency'])\n",
    "# Plot distribution of Monetary\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.distplot(data_process['Monetary'])\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebdbd5-e561-4f45-803a-5868236fed8b",
   "metadata": {},
   "source": [
    "#### 3.2. Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ef6f8-e268-48ba-89d9-a25b4ee9612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(4, 0, -1); f_labels = range(1, 4)\n",
    "# Assign these labels to 4 equal percentile groups \n",
    "r_groups = pd.qcut(data_process['Recency'], q=4, labels=r_labels)\n",
    "# Assign these labels to 4 equal percentile groups \n",
    "f_groups = pd.qcut(data_process['Frequency'], q=4, labels=f_labels, duplicates='drop')\n",
    "# Create new columns R and F \n",
    "data_process = data_process.assign(R = r_groups.values, F = f_groups.values)\n",
    "data_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d0ec0-0d38-46eb-b293-96639cec9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for MonetaryValue\n",
    "m_labels = range(1, 5)\n",
    "# Assign these labels to three equal percentile groups \n",
    "m_groups = pd.qcut(data_process['Monetary'], q=4, labels=m_labels)\n",
    "# Create new column M\n",
    "data_process = data_process.assign(M = m_groups.values)\n",
    "data_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10f532-564b-44ab-9438-4e693b06f354",
   "metadata": {},
   "source": [
    "#### 3.3. RFM Segmentation\n",
    "\n",
    "Identify Upselling and Cross selling opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbb989-1b48-4be1-81e1-ec42f0ec985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_level(df):\n",
    "    if (df['RFM_Segment'] >= 434 or (df['RFM_Score'] >= 9)) :\n",
    "        return 'VVIP - Can\\'t Loose Them'\n",
    "    elif ((df['RFM_Score'] >= 8) and (df['M'] == 4)):\n",
    "        return 'Champions Big Spenders'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['F'] >= 2)):\n",
    "        return 'Loyal Customers'\n",
    "    elif ((df['RFM_Segment'] >= 221) or (df['RFM_Score'] >= 6)):\n",
    "        return 'Potential Loyalists'\n",
    "    elif (((df['RFM_Segment'] >= 121) and (df['R'] == 1)) or df['RFM_Score'] == 5):\n",
    "        return 'Needs Attention'\n",
    "    elif ((df['RFM_Score'] >= 4) and (df['R'] == 1)):\n",
    "        return 'Hibernating - Almost Lost'\n",
    "    else:\n",
    "        return 'Lost Customers'\n",
    "# Define rfm_level function\n",
    "def rfm_action(df):\n",
    "    if (df['RFM_Segment'] >= 434 or (df['RFM_Score'] >= 9)) :\n",
    "        return 'No Price Incentives; Offer Limited edition and Loyality programs'\n",
    "    elif ((df['RFM_Score'] >= 8) and (df['M'] == 4)):\n",
    "        return 'Upsell most expensive items'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['F'] >= 2)):\n",
    "        return 'Loyality programs;Cross Sell'\n",
    "    elif ((df['RFM_Segment'] >= 221) or (df['RFM_Score'] >= 6)):\n",
    "        return 'Cross Sell Recommendations and Discount coupons'\n",
    "    elif (((df['RFM_Segment'] >= 121) and (df['R'] == 1)) or df['RFM_Score'] == 5):\n",
    "        return 'Price incentives and Limited time offer'\n",
    "    elif ((df['RFM_Score'] >= 4) and (df['R'] == 1)):\n",
    "        return 'Aggressive price incentives'\n",
    "    else:\n",
    "        return 'Don\\'t spend too much trying to re-acquire'\n",
    "# Create a new variable RFM_Level\n",
    "rfm['RFM_Segment'] = rfm.RFM_Segment.apply(lambda x: int(x))\n",
    "rfm['Customer Segment'] = rfm.apply(rfm_level, axis=1)\n",
    "# Create a new variable RFM_Level\n",
    "rfm['Marketing Action'] = rfm.apply(rfm_action, axis=1)\n",
    "rfm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c620660-0810-4048-aa9c-5c7cdfa823f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for each Customer Segment, and return a size of each segment \n",
    "rfm_level_agg = rfm.groupby('Customer Segment').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': ['mean', 'count'],\n",
    "    'Marketing Action': 'unique'\n",
    "}).round(1)\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e1676-dfbe-42a1-bf9f-d5aa61f1534f",
   "metadata": {},
   "source": [
    "* Actions:\n",
    "From here, we can see that a sufficient percentage (~45%) of our customers are in the top tier RFM levels. Olist must be doing something right to be maintaining their loyalty! The other 55% will need some work.\n",
    "\n",
    "1. Potential Loyalists —\n",
    "High potential to enter our loyal customer segments, why not throw in some freebies on their next purchase to show that you value them!\n",
    "\n",
    "2. Needs Attention —\n",
    "Showing promising signs with quantity and value of their purchase but it has been a while since they last bought sometime from you. Let’s target them with their wishlist items and a limited time offer discount.\n",
    "\n",
    "3. Hibernating Almost Lost —\n",
    "Made some initial purchase but have not seen them since. Was it a bad customer experience? Or product-market fit? Let’s spend some resource build our brand awareness with them.\n",
    "\n",
    "4. Lost Customers —\n",
    "Poorest performers of our RFM model. They might have went with our competitors for now and will require a different activation strategy to win them back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a3115-479c-4c4a-9384-f46e2c0ba979",
   "metadata": {},
   "source": [
    "#### 3.4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f55477-f3ae-40de-bcea-69bd53b5b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_level_agg.columns = rfm_level_agg.columns.droplevel()\n",
    "rfm_level_agg.columns = ['RecencyMean','FrequencyMean','MonetaryMean', 'Count', 'MarketingAction']\n",
    "#Create our plot and resize it.\n",
    "fig = plt.gcf()\n",
    "ax = fig.add_subplot()\n",
    "fig.set_size_inches(16, 10)\n",
    "squarify.plot(sizes=rfm_level_agg['Count'], \n",
    "              label=['Champions Big Spenders',\n",
    "                     'Hibernating - Almost Lost',\n",
    "                     'Lost Customers',\n",
    "                     'Loyal Customers',\n",
    "                     'Needs Attention',\n",
    "                     'Potential Loyalists',\n",
    "                     'VVIP - Can\\'t Loose Them'], alpha=.6 )\n",
    "plt.title(\"RFM Segments\",fontsize=40,fontweight=\"bold\")\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c24447-b726-4ed9-8e84-bceb646d21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or interactive version\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig =go.Figure(go.Treemap(\n",
    "    labels = rfm_level_ag['Customer Segment'],\n",
    "    parents = ['Customer Segmentation', 'Customer Segmentation', 'Customer Segmentation', 'Customer Segmentation', 'Customer Segmentation', 'Customer Segmentation', 'Customer Segmentation'],   #rfm_level_ag[('Marketing Action', 'unique')].tolist(), \n",
    "    values= rfm_level_ag[('Monetary', 'count')]\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2c414-0db9-431a-ba95-74234933e7e3",
   "metadata": {},
   "source": [
    "We can form our Marketing campaigns and targeting Strategies based on the above Customer Segmentation.\n",
    "\n",
    "- We can Upsell high end products to Big Spenders\n",
    "- We can Cross Sell complimentary products to Loyal and Best Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf9070-8190-4cf5-8caf-a3825d5a1139",
   "metadata": {},
   "source": [
    "## 4. K-Means clustering - Optional\n",
    "\n",
    "We have carefully crafted the clusters based on the RFM score and segmented customers. Now lets apply a Machine Learning approach to identify if there are any hidden segments we can find from clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39563bb-59c5-4544-8b38-210e32af21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rfm = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "data_rfm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418a444-7305-4b9e-8976-fbc563d0ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_log = np.log(data_rfm)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_log)\n",
    "data_normalized = scaler.transform(data_log)\n",
    "data_normalized = pd.DataFrame(data=data_normalized, index=data_rfm.index, columns=data_rfm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cb3bd-24b6-420c-9611-0d19bdbd0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=1) \n",
    "kmeans.fit(data_normalized)\n",
    "cluster_labels = kmeans.labels_\n",
    "data_rfm_k5 = data_rfm.assign(Cluster=cluster_labels)\n",
    "grouped = data_rfm_k5.groupby(['Cluster'])\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df6a14-aaf4-4acc-b370-f20d00a5062a",
   "metadata": {},
   "source": [
    "To find which ‘k’ value is more suitable for our data we will use elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef804fe5-a24c-44d0-aece-b43086ebdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for k in range(1, 8):  \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    kmeans.fit(data_normalized)\n",
    "    sse[k] = kmeans.inertia_\n",
    "plt.figure(figsize=(18,9))\n",
    "\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416ad17-ddae-4813-8711-62df1bcb22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=1) \n",
    "kmeans.fit(data_normalized)\n",
    "cluster_labels = kmeans.labels_\n",
    "data_rfm_k4 = data_rfm.assign(Cluster=cluster_labels)\n",
    "grouped = data_rfm_k4.groupby(['Cluster'])\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018a494-1247-46a2-b673-6489500d2069",
   "metadata": {},
   "source": [
    "#### 4.1. Calculate relative importance of each attribute\n",
    "Now we will calculate the relative importance of the RFM values within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc922956-1016-4848-9d03-c0b41bd68572",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_avg = data_rfm_k4.groupby(['Cluster']).mean() \n",
    "population_avg = data_rfm.mean()\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "relative_imp.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe9b46-49dd-4cda-ab99-dbff517e1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 5))\n",
    "plt.title('Relative importance of attributes')\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011addf-da7d-44e0-a540-bda5b386a066",
   "metadata": {},
   "source": [
    "Relative Importance of RFM among K-Means Clusters\n",
    "We can see that our grouped summary of the mean of R, F, M that each cluster of customers places a different emphasis on our 4 features:\n",
    "\n",
    "- Cluster 0\n",
    "It has the highest MontaryValue mean and low Recency mean and the highest frequency mean — This is our ideal customer segment\n",
    "\n",
    "- Cluster 1\n",
    "It performs poorly across R, F, and M. we will need to design campaigns to activate them again.\n",
    "\n",
    "- Cluster 2\n",
    "They shopped with us recently but have not spend as much or as frequently as we would like them to — perhaps some personalization of products targeted at them can help to maximize their lifetime-value and come back to purchase?\n",
    "\n",
    "- Cluster 3\n",
    "It has spent quite a fair amount with us but has not shopped with us in the 3–4 months — We will need to do something before we lose them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8567533-56b4-4b13-8fd8-82881b3ffc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
